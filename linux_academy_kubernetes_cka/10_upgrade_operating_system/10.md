# Upgrade operating system inside cluster
- what if node has pods running on it? how to safely handle downtime?
1. move pods off the node being changed to be safe
- if node is rebooted and downtime is brief, kubelet will try to restart pod on that node
- if downtime > 5 min, node controller will terminate pods completely
    - therefore important to have deployment or replicaset
        - this will schedule pod to new node
1. check if pods running on node: `kubectl get nodes -o wide` 
2. evict pod from node:  `kubectl drain <node_name> --ignore-daemonsets`
    - this doesn't work for `daemon sets` like `flannel` and `kube-proxy`
3. get pods and see pod has moved to new node: `kubectl get nodes -o wide` 
4. view nodes and see `SchedulingDisabled`, meaning no pods will be scheduled to it: `kubectl get nodes -o wide` 
5. take down server, do maintenance, and bring back online w/ `kubectl uncordon <node_name>`

# decomission server entirely
1. drain: `kubectl drain <node_name> --ignore-daemonsets`
2. remove from cluster entirely: `kubectl delete node <node_name>`
3. delete the server on AWS or wherever

# add new server
1. create server on AWS or wherever
2. log into server w/ ssh or whatever
3. install `docker`, `kubeadm`, `kubectl`, and `kubectl`
4. add server to cluster:
- need to generate new token to add node to cluster
    1. from master: `sudo kubeadm token generate`
    2. generate join command `sudo kubeadm token create <token_generated> --ttl 2h --print-join-command`
        - generates something like `kubeadm join 423.543.1232.44:1234 --token fadsfsa --discovery-token-ca-cert-hash sha256:9ew3i2jefwdskadskfasdk`
    3. copy output and bring to new server `sudo <output>`
5. verify node added correctly: `kubectl get nodes`
