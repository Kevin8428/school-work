# security primitives
- to authenticate with api-server, request goes thru steps to:
    - authenticate
    - authorize
    - emit request
- api-server evaluates if request is coming from `service account`, or `normal user account`
    - `normal user account`:
        - privagte key, user-store, or file w/ usernames and passwords
- kubernetes does not have objects that represent normal user accounts
- normal users cannot be added to the cluster thru an API call
- `service account` - used to manage identity of requests coming into api-server from pods
    - `kubectl get serviceaccounts`
        - `kubectl get sa`
            - short for `kubectl get serviceaccount` (singular)
        - see `default` - created when cluster is created
    - `kubectl create serviceaccount <some_name>`
        - will create a service account and a secret for the service account

## secret
- contains: 
    - the public certificate authority of api-server 
    - the signed JWT

1. get service account
    - `kubectl get sa <some_name> -o yaml`
    - get secret name from output
2. view ecret
    - `kubectl get secret <scret_name>`
**the secret found here is what the request will use to authenticate with the api-server**
- can assign a service account to a pod inside the pod manifest
    - if not assigned, will use default service account
    - every pod has a service account

1. specify service account in yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  serviceAccountName: jenkins
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
```
2. can now go to jenkins server, add k8s cli-plugin
3. add token
4. now jenkins server can control pods using service account token

NOTE when using `kubectl` - need to note location of cluster and have credentials to access it
1. check location/credentials
    - `kubectl config view`
        - shows server address, cluster context, user
2. view config in .kube directory

- each cluster, user, and context use same name
    - name is used to refer to context, user, or cluster
- if you want to access cluster from different server need to pass in cluster location, user, and context

- want to let Chad access cluster remotely
1. on master server, set credentials
    - `kubectl config set-credentials chad --username=chad --password=password`
2. create new cluster role binding
    - `kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous`
        - don't do this in production
3. copy certificate file over to his machine
    - `scp ca.crt cloud_user@12.34.567.890:~/`
        - don't do this in production
4. from remote work-station: set cluster location, credentials, and context
    - set cluster location
        - get server_address from master kube config
        - `kubectl config set-cluster kubernetes --server=<server_address> --certificate-authority=ca.cert --embed-certs=true`
    - set credentials
        - `kubectl config set-credentials chad --username=chad --password=password` 
    - create context
        - used to connect multiple clusters
        - `kubectl config set-context kubernetes --cluster-kubernetes --user=chad --namespace=default`
    - use context
        - `kubectl config use-context kubernetes`
    - run commands from remote
        - `kubectl get nodes` - should cluster nodes from remote station
