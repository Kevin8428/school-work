# build high availability and fault tolerance

- add additional replicated components
    - along with keeping replicas of app on different nodes, and additional nodes
    - protect against failure of kube-system components by replicating them
- each component of control plane can be replicated, but some components must be in stand-by state
    - to avoid conflict with replicated components

## scheduler and controller-master replication
- because these actively watch cluster state, and take action when it changes, can't run 2 instances in tandem
    - they'd be competing to change state and could create duplicate resources
    - so only 1 active at a time
- SET ACTIVE OR STANDBY: controlled by `leader-elect option`
    - leader is selected as the active component, set to true by default
        - so each will be aware of each other and when they are leader
- leader election is build by creating endpoint-resource seen in `kube-scheduler` yaml
    - annotation in yaml saying if instance is leader. Has field `holder-identity`
- once leader, must periodically update resource. By default updates every 2 seconds

## etcd replication
- requires special instructions to replicate
- is distributed, so they can be in stacked or external etcd topology
- `stacked` - each control plane node creates a local etcd member
    - this member communicates only with kube api-server of this node 
- `external` - etcd is external to kubernetes cluster. 
- etcd uses a `raft consensus algorithm` - requires majority to progress to next state
    - more than half the nodes must be taking part in state change
        - therefore to have a majority you `must have an odd number of etcd instances`
        - having 2 etcd instances is worse than having 1
- `even in huge clusters, never need more than 7 etcd instances`
- install etcd then other components