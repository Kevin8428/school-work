# taints
- nodes are tainted in order to repel work
- master node is tainted "no schedule"
- `kubectl describe node <name>`
    - "master:NoSchedule"

# toleration
- tolerate a taint
- can be put in pod yaml
- could put toleration to allow scheduling to master node, if you wanted
- `kubectl get pods -n kube-system` - get kube-proxy pod name
- `kubectl get pods <kube-proxy-name> -n kube-system -o yaml`
    - see toleration
        - see effect:noSchedule; key: node.kuubernetes.io/unschedulable
            - this pod will tolerate a node that is unschedulable
                - eg the pod will schedule, even if the node says it's unschedulable
                - true for kube-proxy pod becuause it's a daemonset pod, needs to run on every node. 

- pod will not be scheduled to a node that is tainted, unless it has a toleration for that node

# CPU and Memory
- scheduler doesn't look at each resource to get best node
- scheduler gets sum of resources requested by existing pods on that node
    - bc pod might not be using full resources at any given time
- `least requested priority` - prefer nodes w/ least resources requested, to spread pods evenly
- `most requested priority` - prefer node w/ most resources
    - good if you're paying for each node. Can pack pods into fewest amount of machines
- configure priority in scheduler

- set node priority
    - configure in scheduler

- `kubectl describe nodes` - find node capacity
    - shows capacity and allocatable

# schedule pod
- create yaml:
```
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod1
    resources:
      requests:
        cpu: 800m
        memory: 20Mi
```
- nodeSelector - specify pod 3
- `kubectl create -f resource-pod1.yaml` - build
- change cpu to 1000m and create second pod
- `kubectl describe pods <pod_name>` - inspect new build to see if/why it failed

- pods share CPU of node, if 1 is idle, other consumes available CPU if needed
    - if both active and using requested CPU, remaining excell CPU will be split

**if memory is requested, it's possible for a pod to take all memory on node**
- solution: limits
- if no request set, but limit is set - request == limit
- limits can go beyond total utilization of CPU and memory. Request cannot.
    - CPU or memory limits can be above node CPU or memory
- `kubectl exec -it <pod_name> top`
    - view CPU, memory etc
    - `top` command gets memory amount from node, not container
        - can set limit to container, but container is never aware of the limit


