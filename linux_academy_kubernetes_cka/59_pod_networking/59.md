# pod networking

## requirements
- every pod should have an IP address
- every pod can talk to every pod in the node
- every pod can talk to every pod on other nodes WITHOUT NAT

- flannel is one solution

## solve without flannel
- have 3 node cluster, each are same
- nodes are part of external network
- all on 192.168.1.x series

# nat gateway

# routing table
`route` - view routing table
- `destination`
- `gateway`
- `genmask`


PRACTICE DOING THIS (video 166 udemy):
1. create bridge network on each node
    - `ip link add v-net-0 type bridge`
2. bring it up
    - `ip link set dev v-net-0 up`
3. assign ip address to bridge network
    - each bridge network on own subnet
    - `10.244.1.0/24`
    - `10.244.2.0/24`
    - `10.244.3.0/24`
4. set ip address for bridge interface
    - `ip addr add 10.244.1.1/24 dev v-net-0`
    - `ip addr add 10.244.2.1/24 dev v-net-0`
    - `ip addr add 10.244.3.2/24 dev v-net-0`
5. need to do the following for each/every container:
    - this connects container to internal networks
    - after this, containers can talk to each other
    - attach container to network
        - create veth pair
        - `ip link add ...`
    - attach 1 end to container
        - `ip link set ...`
    - attach 1 end to bridge
        - `ip link set ...`
    - assign ip address
        - `ip -n <namespace> addr add`
    - add route to default gateway
        - `ip -n <namespace> route add`
    - bring up interface
        - `ip -n <namespace> link set`
6. enable pods to reach pods on other nodes
    - `10.244.1.2` - pod A on container 1
    - `10.244.2.2` - pod B on container 2
    - A can't locate B because on different network
    - so A routes to node 1 ip as it's default gateway
    - node 1 doesn't know pod B ip bc pod B IP is a privat network on node 2
    1. add route to node 1 routing table to route traffic to pod B via node 2 ip
        - from pod A:
        - `ip route add <pod_b_ip> via <node_2_ip>`
        - `ip route add 10.244.2.2 via 192.168.1.12`
        - now pod A can ping `10.244.2.2` !!
    2. add routes on all hosts (nodes) to all other hosts/pods

ABOVE GETS COMPLICATED
- do all on router on network
- point all hosts to use this as default gateway
    - `easily manage routes to all networks on routing table on router`
- ^^ better solution
- individual networks now form larger network with new ip `10.244.0.0/16`

# CNI
- need to run script very time a container is created, to connect container to network
- CNI runs this script on startup of a container
- says: `this is how you should call a script ever time a container is created`

CNI file spec:
- ADD section
- DELETE section
```
ADD)
ip -n <namespace> link set...
DEL)
ip link del...
```

- kubelet on each node is responsible for creating a container
- kubelet looks at CNI configuration
    - id script name
    - look in /etc/cni/bin for script
    - execute with `./script.sh add <container> <namespace>`


# Build network on linux academy cluster
3 node cluster
1. build bridge on each node `sudo ip link add v-net-0 type bridge`
2. bring up `sudo ip link set dev v-net-0 up`
3. view: `ip addr`
```
v-net-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether a2:5a:fc:87:da:fa brd ff:ff:ff:ff:ff:ff
    inet6 fe80::a05a:fcff:fe87:dafa/64 scope link
       valid_lft forever preferred_lft forever
```
4. assign ip to bridge interfaces -- DID WE ACTUALLY SET ANYTHING HERE?
    - we decide to put each bridge network on own subnet
        - can choose any private address range
        - eg 10.244.1.x and  10.244.2.x and 10.244.3.x
5. set ip address for bridge interface: `sudo ip addr add 10.244.x.1/24 dev v-net-0`
```
v-net-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether a2:5a:fc:87:da:fa brd ff:ff:ff:ff:ff:ff
    inet 10.244.3.1/24 scope global v-net-0
       valid_lft forever preferred_lft forever
    inet6 fe80::a05a:fcff:fe87:dafa/64 scope link
       valid_lft forever preferred_lft forever
```
- `inet` is the internet address, likely address needed to get to or from subnet

### the following steps need to be done for every container, so we can create a script for it
- allow pods to talk on their own node
- name `net-script.sh`
1. create veth pair - build bridge
    - need `pipe` or `virtual cable`
    - `sudo ip link add bb1 type bridge`
2. attach veth pair - attach ends to container and bridge
    - `ip link set` eg `ip link set veth-red netns red`
    - `sudo ip link set dev bb1 up` - attach one end to bridge
    - `sudo ip link set dev bb1 up` - attach one end to container
3. assign ip address
    - `ip -n <naamespace> addr add ...`
    - `sudo ip addr add <container_ip> dev <interface>`
    - `sudo ip addr add 10.244.2.2 dev bb1`
4. add route to default gatewaay
    - `ip -n <naamespace> route add ...`
    - what ip? add ourselves or store in DB
    - for now assume it is `10.244.1.2` - free IP
5.  bring up interface
    - `ip -n <namespace> link set ...`

- run script for all containers to get containers connected to network
    - containers can now talk
- run script on all nodes to assign IP address' and connect to own internal network

CONNECT CONTAINERS USING BRIDGES:
https://dzone.com/articles/step-by-step-guide-establishing-container-networki?utm_medium=feed&utm_source=feedpress.me&
utm_campaign=Feed:%20dzone

video:
https://www.youtube.com/watch?v=6v_BDHIgOY8
- look at 11min  mark

`sudo ip link add veth-bb1 type veth peer name veth-bb2` - creaate
`sudo ip link set veth-bb1 up` - up, but need to attach
`sudo ip link set veth-bb2 up` - up, but need to attach


ip address add <ip_address>/24 dev <veth_name>
ip link set veth-sfo up


CONNECT CONTAINER TO VETH
`ip addr add <contianer_ip>/24 dev veth_1` - add container ip to VETH
`ip link set veth_1 up` - run veth

CONNECT BRIDGE TO VETH - DOES THIS WORK?
`ip addr add <bridge_ip>/24 dev veth_2` - add container ip to VETH
`ip link set veth_2 up` - run veth


$ sudo ip addr add 172.12.0.11/24 dev br0
$ sudo ip link set br0 up



1. create veth pair
- `ip link add veth-bb1 type veth peer name veth-bb2`
2. add container to veth end 2
- `addr add 10.244.2.2/24 dev veth-bb2`
- `ip link set veth-bb2 up`
3. add bridge to veth end 1
- `addr add 10.244.2.1/24 dev veth-bb1`
- `ip link set veth-bb1 up`
4. 
- ``
5. 
- ``


1. `ip netns add red`
1. `ip netns add blue`



<!--  -->
1. `ip netns add ns1` 
- create ns
2. `ip net ns add ns2` 
- create ns
3. `ip link add bb-veth type veth peer name bb-veth-br`
- create veth to connect to bridge
4. `ip link set bb-veth netns ns1`
- attach veth to container thru ns1
5. `ip link set bb-veth netns ns1`
- attach veth to bridge

1. `ip link add bridge-1 type bridge` 
- create bridge
- from server off node worker-1 
2. `ip link set dev bridge-1 up` 
- start bridge
- from server off node worker-1 
3. `ip addr add 10.255.1.1/24 dev bridge-1` 
- assign ip to bridge
- from server 
4. `ip link add bb-veth type veth peer name bb-veth-br`
- create veth to connect to bridge
5. `ip link set bb-veth netns ns1`
- attach ns to pipe end bb-veth
6. `ip link set bb-veth-br master bridge-1`
- attach bridge to pipe end bb-veth-br
7. `ip -n ns1 addr add 192.168.15.1 dev bb-veth`
- attach ip address to ns1
8. `ip -n ns1 addr add 192.168.15.2 dev bb-veth`
- attach ip address to ns1
9. `ip -n ns1 link set bb-veth up`
- start devices



for each container:
4. `ip link add` - create veth pipe
5. `ip link set` - attach to container
6. `ip link set` - attach to bridge
now containers should be able to ping each other
<!--  -->


160 video
1. create bridge
    - `ip link add v-net-0 type bridge` 
2. UP the bridge
    - `ip link set dev v-net-0 up`
- bridge is an interface for host and switch for namespace
3. connect to bridge
    - `ip link add bb-veth type veth peer name bb-veth-br`
    - `ip link add bb2-veth type veth peer name bb2-veth-br`
4. set ip address for bridge
    - `ip addr add 10.244.1.1 dev v-net-0`

- `route` - view routing table
- `arp` - view arp table
- connect 2 namespaces using `veth`
- `ip link add <one> type veth peer name <two>` - create veth
- `ip link set <one> <type> <id/namespace/etc>` - attach interfaces
    - `ip link set `
- `ip addr add <ip> dev <id/name>` - assign ip