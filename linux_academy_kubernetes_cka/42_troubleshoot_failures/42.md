# troubleshoot application failures
- can show reason why container terminated in pod status using `termination message`

```
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - image: busybox
    name: main
    command:
    - sh
    - -c
    - 'echo "I''ve had enough" > /var/termination-reason ; exit 1'
    terminationMessagePath: /var/termination-reason
```
- write 'I've had enough' to /var/termination-reason

- can add to liveness probe
```
apiVersion: v1
kind: Pod
metadata:
  name: liveness
spec:
  containers:
  - image: linuxacademycontent/candy-service:2
    name: kubeserve
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8081
```

## debug guide:
1. check all `kubectl get pods` - any status say "error"?
2. `kubectl describe <pod_name>`
    - look at `events` section
    - look at `state` and `reaason`
3. check logs `kubectl logs <pod_name>`
4. Export the YAML of a running pod, in the case that you are unable to edit it directly:
    - `kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml`
        - give all except status
5. delete pod w/ error
6. redeploy pod using yaml file
