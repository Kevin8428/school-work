# scheduler
- assign a pod to a node based on resource requirements of the pod
- scheduling rules put in place by default, but can override
    - say want all pods to same node
    - maybe want pods to host w/ solid state drive
    - etc

## scheduler steps
- scheduler goes thru many checks for finding best node:
    1. identify if node has adequate resources
    2. check if node is running out of resources
    3. check if pod requests to be scheduled by name
    4. check if node has specific label that matches node-selector in pod spec
    5. check if pod is requesting to be bound to a specific host port, and if so, does node have that port available?
    6. is pod requesting certain type of volume? if so, can volume be mounted? are other pods using the same volume?
    7. does pod tolerates the taints of the node. Master node is tainted with no schedule - no pods scheduled to it
    8. is pod specifying pod/node affinit rules - would scheduling break rules
    
    - might result in more than 1 option. Scheduler prioritizes. if all same priority, choose in round-robin.
        - can override priority w/ affinity rules

## affinity example
- 2 nodes
1. give nodes labels for respective AZ
    - `kubectl label node <node_1_name> availability-zone=zone1`
    - `kubectl label node <node_2_name> availability-zone=zone1`
2. specify dedicated vs shared
    - `kubectl label node <node_1_name> shared-type=dedicated`
    - `kubectl label node <node_2_name> shared-type=shared`
3. make deployment spec
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pref
spec:
  selector:
    matchLabels:
      app: pref
  replicas: 5
  template:
    metadata:
      labels:
        app: pref
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          # ^ anything under fields are rules for what labels/nodes must have to be scheduled. rules don't affect pods already on node.
          - weight: 80 - prefer schedule to zone 1
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20 - also prefer schedule to zone 2, but less important
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
```
4. run deployment
`kubectl create -f deployment.yaml`
5. view
`kubectl get deployments`
6. view pods
`kubectl get pods`
- should see 80% of the 6 pods are on pod w/ label zone1

NOTE:
- scheduler also uses `selector-spread` priority function
    - makes sure pods belonging to same replicaSet are spread to different nodes to prevent downtime